# Assignment 5: Self-attention, Transformers and Pretraining

## 1. Attention exploration

[TODO]

## 2. Pretrained Transformer models and knowledge access
- [colab notebook](https://colab.research.google.com/drive/1eUwaEZFWhOMjUtDTKKPcaUjOGJHrzixf?usp=sharing)

- [x] c. Implement finetuning (without pretraining)
- [x] d. Make predictions (without pretraining).
    - Model's accuracy on dev set: `Correct: 8.0 out of 500.0: 1.6%`
    - `london_baseline`: `Correct: 25.0 out of 500.0: 5.0%`

- [ ] e. Define aspan corruptionfunction for pretraining.
- [ ] f. Pretrain, finetune, and make predictions
- [ ] g. Research! Write and try out thesynthesizervariant
